{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IRE_bert_zenodo_per_user.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zltJ9SOxHuQJ"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3KnM5JtNLc8"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "import calendar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guB0JwmbU7d0"
      },
      "source": [
        "# create bert model\n",
        "\n",
        "from transformers import BertConfig\n",
        "from transformers import TFBertForSequenceClassification\n",
        "\n",
        "model_name = 'bert-base-uncased'\n",
        "config1 = BertConfig.from_pretrained(model_name)\n",
        "config1.num_labels=2\n",
        "\n",
        "bert_model = TFBertForSequenceClassification.from_pretrained(model_name,config = config1)\n",
        "learning_rate = 2e-5\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "bert_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "# model.get_layer('bert').trainable=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cMBnvJrVPk-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bfdbc21-67dc-4c23-894e-d4e9ed3e0c45"
      },
      "source": [
        "# Train just the bert\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split( bert_posts, bert_labels, test_size=0.3, random_state=21)\n",
        "\n",
        "bert_train_dataset = encode_examples(X_train,y_train)\n",
        "bert_test_dataset = encode_examples(X_test,y_test)\n",
        "print(len(bert_train_dataset))\n",
        "bert_train_dataset_batched = bert_train_dataset.batch(batch_size=5)\n",
        "bert_test_dataset_batched = bert_test_dataset.batch(batch_size=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VTnHHYvhTMd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab363c55-18db-4c86-85f9-839410bc7ea7"
      },
      "source": [
        "bert_history = bert_model.fit(bert_train_dataset_batched,epochs=4,verbose=1,validation_data=bert_test_dataset_batched)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "1893/1893 [==============================] - 198s 105ms/step - loss: 0.6499 - accuracy: 0.6260 - val_loss: 0.6227 - val_accuracy: 0.6544\n",
            "Epoch 2/4\n",
            "1893/1893 [==============================] - 197s 104ms/step - loss: 0.5716 - accuracy: 0.7151 - val_loss: 0.6478 - val_accuracy: 0.6484\n",
            "Epoch 3/4\n",
            "1893/1893 [==============================] - 200s 105ms/step - loss: 0.4350 - accuracy: 0.8117 - val_loss: 0.8929 - val_accuracy: 0.6467\n",
            "Epoch 4/4\n",
            "1893/1893 [==============================] - 199s 105ms/step - loss: 0.3170 - accuracy: 0.8746 - val_loss: 1.0662 - val_accuracy: 0.6415\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxYvcM2TkSUF"
      },
      "source": [
        "res = np.argmax(bert_model.predict(bert_test_dataset_batched),axis = -1)\n",
        "lebl = []\n",
        "for data in bert_test_dataset_batched:\n",
        "  lebl.extend(tf.squeeze(data[1]).numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGvlpPCRmQqI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1277af10-cf0d-4e77-936e-83f8f0a2a4d9"
      },
      "source": [
        "print(res[0][:100])\n",
        "lebl = []\n",
        "for i,data in enumerate(bert_test_dataset_batched):\n",
        "  lebb = np.squeeze(data[1])\n",
        "  if i!= len(bert_test_dataset_batched) -1:\n",
        "    lebl.extend(list(lebb))\n",
        "print(lebl[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 1 1 0 0\n",
            " 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 1 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0]\n",
            "[1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug13019RZeOK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0225fea-8d39-45e1-c502-d4ae1178c46d"
      },
      "source": [
        "bert_history = bert_model.fit(bert_train_dataset_batched,epochs=4,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "2700/2700 [==============================] - 242s 89ms/step - loss: 0.6726 - accuracy: 0.5799\n",
            "Epoch 2/4\n",
            "2700/2700 [==============================] - 247s 91ms/step - loss: 0.6337 - accuracy: 0.6490\n",
            "Epoch 3/4\n",
            "2700/2700 [==============================] - 247s 92ms/step - loss: 0.5601 - accuracy: 0.7255\n",
            "Epoch 4/4\n",
            "2700/2700 [==============================] - 247s 92ms/step - loss: 0.4393 - accuracy: 0.8079\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gnt0aybx_KOl"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "max_length_text = 50\n",
        "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
        "  return {\n",
        "      \"input_ids\": input_ids,\n",
        "      \"token_type_ids\": token_type_ids,\n",
        "      \"attention_mask\": attention_masks,\n",
        "  },label\n",
        "\n",
        "def encode_examples(posts,labels):\n",
        "  # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
        "  input_ids_list = []\n",
        "  token_type_ids_list = []\n",
        "  attention_mask_list = []\n",
        "  label_list = []\n",
        "  for post,label in zip(posts,labels):\n",
        "    if not isinstance(post, str):\n",
        "      continue\n",
        "    bert_input = tokenizer.encode_plus(post,\n",
        "                                      truncation=True,           \n",
        "                                      add_special_tokens = True, # add [CLS], [SEP]\n",
        "                                      max_length = max_length_text, # max length of the text that can go to BERT\n",
        "                                      padding='max_length', # add [PAD] tokens\n",
        "                                      return_attention_mask = True, # add attention mask to not focus on pad token\n",
        "                                      )\n",
        "  \n",
        "    input_ids_list.append(bert_input['input_ids'])\n",
        "    token_type_ids_list.append(bert_input['token_type_ids'])\n",
        "    attention_mask_list.append(bert_input['attention_mask'])\n",
        "    label_list.append([label])\n",
        "  \n",
        "  return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)\n",
        "\n",
        "def convert_to_tfdataset(users_posts,labels):\n",
        "  datasets = []\n",
        "  for user_posts,label in zip(users_posts,labels):\n",
        "    datasets.append( (encode_examples(user_posts,label)) )\n",
        "  \n",
        "  # dataset = tf.stack(datasets)\n",
        "  return datasets\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmWpN9nIY706"
      },
      "source": [
        "dataset = convert_to_tfdataset(posts,lables)\n",
        "total_users = len(dataset)\n",
        "test_users = int(0.3 * total_users)\n",
        "train_users = total_users - test_users\n",
        "train_dataset = dataset[:train_users]\n",
        "test_dataset = dataset[train_users:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kiajP7u4M_d"
      },
      "source": [
        "# # Dataset\n",
        "# from transformers import BertTokenizer\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "# max_length_test = 50\n",
        "# def map_example_to_dict_prediction(input_ids, attention_masks, token_type_ids):\n",
        "#   return {\n",
        "#       \"input_ids\": input_ids,\n",
        "#       \"token_type_ids\": token_type_ids,\n",
        "#       \"attention_mask\": attention_masks,\n",
        "#   }\n",
        "\n",
        "# def encode_examples_prediction(posts):\n",
        "#   # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
        "#   input_ids_list = []\n",
        "#   token_type_ids_list = []\n",
        "#   attention_mask_list = []\n",
        "#   for post in posts:\n",
        "#     if not isinstance(post, str):\n",
        "#       continue\n",
        "    \n",
        "#     bert_input = tokenizer.encode_plus(post,\n",
        "#                                        truncation=True,           \n",
        "#                                       add_special_tokens = True, # add [CLS], [SEP]\n",
        "#                                       max_length = max_length_test, # max length of the text that can go to BERT\n",
        "#                                       padding='max_length', # add [PAD] tokens\n",
        "#                                       return_attention_mask = True, # add attention mask to not focus on pad token\n",
        "#                                       )\n",
        "#     input_ids_list.append(bert_input['input_ids'])\n",
        "#     token_type_ids_list.append(bert_input['token_type_ids'])\n",
        "#     attention_mask_list.append(bert_input['attention_mask'])\n",
        "#   return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list)).map(map_example_to_dict_prediction)\n",
        "\n",
        "# def convert_to_tfdataset(posts,labels,batch_size = 30):\n",
        "#   dataset = encode_examples(posts,labels)\n",
        "#   dataset = dataset.shuffle(dataset.__len__().numpy()).batch(batch_size)\n",
        "#   return dataset\n",
        "\n",
        "# # split\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# X_train, X_test, y_train, y_test = train_test_split( posts, lables, test_size=0.2, random_state=21)\n",
        "# train_dataset = convert_to_tfdataset(X_train,y_train,batch_size = 30)\n",
        "# test_dataset = convert_to_tfdataset(X_test,y_test,batch_size = 30)\n",
        "\n",
        "# # Train bert normally\n",
        "# drive_models = 'drive/MyDrive/ire_bert/'\n",
        "# # bert_history = bert_model.fit(train_dataset,epochs=10,verbose=1,validation_data=test_dataset)\n",
        "# # bert_model.save_weights(drive_models+'bert_zenodo')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocBtSd3IMPy7"
      },
      "source": [
        "class CRF_Model(tf.keras.Model):\n",
        "  def __init__(self,N_LABELS):\n",
        "    super(CRF_Model, self).__init__()\n",
        "    self.emission_layer = bert_model\n",
        "    initializer = tf.keras.initializers.GlorotUniform()\n",
        "    self.transition_params = tf.keras.backend.variable( initializer([N_LABELS, N_LABELS]),name='transition_params')\n",
        "\n",
        "  def call(self,input):\n",
        "    # mask = tf.sequence_mask(inputs[1], tf.shape(inputs[0])[1])\n",
        "    input = list(input)\n",
        "    emissions = []\n",
        "    for i in range(len(input)):\n",
        "      for item in input[i]:\n",
        "        input[i][item] = tf.expand_dims(input[i][item],axis = 0)\n",
        "      emissions.append(self.emission_layer(input[i]))\n",
        "    emissions = tf.squeeze(emissions)\n",
        "    return [emissions,self.transition_params]\n",
        "\n",
        "class Trainer(tf.keras.Model):\n",
        "  def __init__(self,modelSpecs):\n",
        "    super(Trainer, self).__init__()\n",
        "    self.modelSpecs = modelSpecs\n",
        "    self.model = CRF_Model(self.modelSpecs['N_LABELS'])\n",
        "\n",
        "  def decode(self,emission, seq_lengths):\n",
        "    viterbi_sequence_list = []\n",
        "    viterbi_score_list = []\n",
        "    for i in range(len(seq_lengths)):\n",
        "      viterbi_sequence, viterbi_score = tfa.text.viterbi_decode(emission[i][:seq_lengths[i]], self.transition_params)\n",
        "      viterbi_sequence_list.append(viterbi_sequence)\n",
        "      viterbi_score_list.append(viterbi_score)\n",
        "    return  viterbi_sequence_list, viterbi_score_list\n",
        "  \n",
        "  def predict(self, posts_seq):\n",
        "    emission_scores = []\n",
        "    for post in posts_seq:\n",
        "      emissions = bert_model(post)\n",
        "      emission_scores.append(emissions[0])\n",
        "    emission_scores = tf.squeeze(emission_scores)\n",
        "    viterbi_sequence, viterbi_score = tfa.text.viterbi_decode(emission_scores, self.model.transition_params)\n",
        "    return viterbi_sequence\n",
        "\n",
        "  def train(self,dataset):\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=self.modelSpecs['learning_rate'])\n",
        "    num_epochs = self.modelSpecs['n_epochs']\n",
        "    logdir=\"logs\"\n",
        "    train_writer = tf.summary.create_file_writer(logdir+'/train')\n",
        "    test_writer = tf.summary.create_file_writer(logdir+'/test')\n",
        "    size = len(dataset)\n",
        "    for epoch in range(num_epochs):\n",
        "      for sample_no,inputs in enumerate(dataset):\n",
        "        labels,tokens = [],[]\n",
        "        for x in inputs.as_numpy_iterator():\n",
        "          labels.append(x[1])\n",
        "          tokens.append(x[0])\n",
        "        # print(inputs, labels, seqLength)\n",
        "        with tf.GradientTape() as tape:\n",
        "          emission,transition_params = self.model(tokens)\n",
        "          loss_value = self.lossfn(emission,labels,transition_params)\n",
        "        tvars = self.trainable_variables\n",
        "        tvars = [v for v in tvars if 'bert' not in v.name]\n",
        "        print(tvars)\n",
        "        grads = tape.gradient(loss_value, tvars)\n",
        "        optimizer.apply_gradients(zip(grads, tvars))\n",
        "        # predictedLabels, score = self.decode(emission, seqLength)\n",
        "        with train_writer.as_default():\n",
        "          tf.summary.scalar('losses', loss_value, step=sample_no+size*epoch)\n",
        "  def call(self,inputs):\n",
        "    # inputs: [emission, transition_params, seq_lengths]\n",
        "    return self.decode(inputs[0],inputs[1],inputs[2])\n",
        "\n",
        "  def lossfn(self, emission, labels, transition_params):\n",
        "    cur_len_labels = len(labels)\n",
        "    cur_len_emissions = len(emission)\n",
        "    MAX_SEQ_LEN = 10\n",
        "    if cur_len_labels < MAX_SEQ_LEN:\n",
        "        paddings_label = (0, MAX_SEQ_LEN - cur_len_labels)\n",
        "        labels = np.pad(labels, paddings_label)\n",
        "    if cur_len_emissions < MAX_SEQ_LEN:\n",
        "        paddings_emission = (( 0, MAX_SEQ_LEN - cur_len_emissions ),(0,0))\n",
        "        emission = np.pad(emission, paddings_emission)\n",
        "    labels = labels[:MAX_SEQ_LEN]\n",
        "    emission = emission[:MAX_SEQ_LEN]\n",
        "    emission = tf.expand_dims(emission, axis = 0)\n",
        "    labels = np.array(labels, dtype = np.int)\n",
        "    labels = tf.expand_dims(labels, axis = 0)\n",
        "    log_likelihood, transition_params = tfa.text.crf_log_likelihood(emission, labels, [cur_len_labels], transition_params)\n",
        "    loss = tf.reduce_mean(-log_likelihood)\n",
        "    return loss\n",
        "\n",
        "\n",
        "modelSpecs = {\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"n_epochs\": 2,\n",
        "    \"N_LABELS\":2\n",
        "}\n",
        "import random\n",
        "random.shuffle(train_dataset)\n",
        "trainer = Trainer(modelSpecs)\n",
        "trainer.train(train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIWXvbZJJaaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "303dc9c4-74bc-4c70-969c-3a587bdc8525"
      },
      "source": [
        "posts = [\"I am depressed\",\"I wanna die\"]\n",
        "posts = [tokenizer(post, return_tensors=\"tf\") for post in posts]\n",
        "print(trainer.predict(posts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrzPGgI6Aetk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "704e03ce-a24f-4677-bcf5-a9429a90c640"
      },
      "source": [
        "modelSpecs = {\n",
        "    \"learning_rate\": 1e-2,\n",
        "    \"n_epochs\": 1,\n",
        "    \"N_LABELS\":2\n",
        "}\n",
        "\n",
        "for x in dataset:\n",
        "  y = x[0]\n",
        "  y = list(y)\n",
        "  for i in range(len(y)):\n",
        "    for item in y[i]:\n",
        "      y[i][item] = tf.expand_dims(y[i][item],axis = 0)\n",
        "    bert_model(y[i])\n",
        "  break\n",
        "\n",
        "r = tokenizer(\"helo this is me\", return_tensors=\"tf\")\n",
        "print(r)\n",
        "# bert_model(r)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[ 101, 2002, 4135, 2023, 2003, 2033,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv29CntpheCr"
      },
      "source": [
        "# df_1 = pd.read_csv('/content/drive/My Drive/Mental Health blog dataset/beyondblue_depression.csv')\n",
        "# df_2 = pd.read_csv( \"/content/drive/My Drive/Mental Health blog dataset/beyondblue_ptsd-trauma.csv\")\n",
        "# df_3 = pd.read_csv('/content/drive/My Drive/Mental Health blog dataset/beyondblue_anxiety.csv')\n",
        "# df_4 = pd.read_csv ('/content/drive/My Drive/Mental Health blog dataset/beyondblue_suicidal-thoughts-and-self-harm.csv')\n",
        "\n",
        "# df_List = [df_1 , df_2, df_3, df_4]\n",
        "# for data_frame in df_List : \n",
        "#   data_frame['index_indi'] = pd.Series ([i for i in range (data_frame.shape[0])])\n",
        "\n",
        "# df = pd.concat(df_List, ignore_index=True)\n",
        "# for i in range (df.date.shape[0]) : \n",
        "#   df.date[i] = datetime.strptime(df.date[i], '%d %B %Y')\n",
        "# df.sort_values(by=['date'], inplace=True)\n",
        "# unique_name = df.author.unique().tolist()\n",
        "# unique_name = np.unique(np.array(unique_name))\n",
        "# post_per_person = []\n",
        "# for k, name in enumerate (unique_name) :\n",
        "#   post_counts = df.loc[df['author'] == name].shape[0]\n",
        "#   post_per_person.append ( [name, int (post_counts)] )\n",
        "\n",
        "# temp = []\n",
        "# for i in range (len(post_per_person)) : \n",
        "#   temp.append (post_per_person[i][1])\n",
        "# sum (temp) / len(post_per_person)\n",
        "# sub_data_names = [] \n",
        "# for i, freq in enumerate (temp) : \n",
        "#   if freq >= 5 and freq <= 50 : \n",
        "#     sub_data_names.append (post_per_person[i][0])\n",
        "\n",
        "# sub_data_set = [] \n",
        "# for name in sub_data_names : \n",
        "#   sub_data_set.append (df.loc[df.author == name])\n",
        "\n",
        "# train_cases = []\n",
        "# for i in range(len ( sub_data_set) ) : \n",
        "#     cat = sub_data_set[i]['category'].to_numpy()\n",
        "#     if 'suicidal-thoughts-and-self-harm' in cat and 'depression' in  cat: \n",
        "#       temp_1 = sub_data_set[i][sub_data_set[i].category == 'suicidal-thoughts-and-self-harm']\n",
        "#       temp_2 = sub_data_set[i][sub_data_set[i].category == 'depression']\n",
        "#       sub_data_set[i] = pd.concat([temp_1, temp_2])\n",
        "#       sub_data_set[i].sort_values(by=['date'], inplace=True)\n",
        "#       train_cases.append (sub_data_set[i])\n",
        "\n",
        "# print('Num of users = ',len(train_cases))\n",
        "# users = [] \n",
        "# labels = [] \n",
        "# for i in range (len(train_cases)) : \n",
        "#   users.append (train_cases[i].post.to_numpy())\n",
        "#   labels.append (train_cases[i].category.to_numpy())\n",
        "\n",
        "# category_labels = []\n",
        "# sequence_length = []\n",
        "# for i in range(len(labels)):\n",
        "#   sequence_length.append(len(labels[i]))\n",
        "#   cat = []\n",
        "#   for label in labels[i]:\n",
        "#     if label == 'suicidal-thoughts-and-self-harm':\n",
        "#       cat.append(0)\n",
        "#     else:\n",
        "#       cat.append(1)\n",
        "#   category_labels.append(cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsbYdZ1pApPq"
      },
      "source": [
        "durl = 'drive/My Drive/zenodo_dataset/'\n",
        "df_1 = pd.read_csv(durl + 'depression.csv')\n",
        "df_2 = pd.read_csv(durl + 'suicidewatch.csv')\n",
        "\n",
        "df_List = [df_1 , df_2]  \n",
        "for data_frame in df_List : \n",
        "  data_frame['index_indi'] = pd.Series ([i for i in range (data_frame.shape[0])])\n",
        "\n",
        "df = pd.concat(df_List, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8HXpW7KEO4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68b25f31-92ef-4967-a142-9e678a416ce2"
      },
      "source": [
        "for i in range (df.date.shape[0]) : \n",
        "  df.date[i] = datetime.strptime(df.date[i], '%Y-%m-%d %H:%M:%S')\n",
        "df.sort_values(by=['date'], inplace=True)\n",
        "unique_name = df.author.unique().tolist()\n",
        "unique_name = np.unique(np.array(unique_name))\n",
        "post_per_person = []\n",
        "for k, name in enumerate (unique_name) :\n",
        "  post_counts = df.loc[df['author'] == name].shape[0]\n",
        "  post_per_person.append ( [name, int (post_counts)] )\n",
        "  \n",
        "temp = []\n",
        "for i in range (len(post_per_person)) : \n",
        "  temp.append (post_per_person[i][1])\n",
        "sum (temp) / len(post_per_person)\n",
        "sub_data_names = [] \n",
        "for i, freq in enumerate (temp) : \n",
        "  if freq >= 5 and freq <= 50 : \n",
        "    sub_data_names.append (post_per_person[i][0])\n",
        "\n",
        "sub_data_set = [] \n",
        "for name in sub_data_names : \n",
        "  sub_data_set.append (df.loc[df.author == name])\n",
        "\n",
        "train_cases = []\n",
        "for i in range(len ( sub_data_set) ) : \n",
        "    cat = sub_data_set[i]['subreddit'].to_numpy()\n",
        "    if 'SuicideWatch' in cat and 'depression' in  cat: \n",
        "      temp_1 = sub_data_set[i][sub_data_set[i].subreddit == 'SuicideWatch']\n",
        "      temp_2 = sub_data_set[i][sub_data_set[i].subreddit == 'depression']\n",
        "      sub_data_set[i] = pd.concat([temp_1, temp_2])\n",
        "      sub_data_set[i].sort_values(by=['date'], inplace=True)\n",
        "      train_cases.append (sub_data_set[i])\n",
        "\n",
        "posts = [] \n",
        "lables = [] \n",
        "for i in range (len(train_cases)) : \n",
        "  posts.append (train_cases[i].post.to_numpy())\n",
        "  lables.append (train_cases[i].subreddit.to_numpy())\n",
        "\n",
        "print(len( post_per_person))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "73556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZs3G9yP02xu"
      },
      "source": [
        "for label_seq in lables:\n",
        "  for i in range(len(label_seq)):\n",
        "    if label_seq[i] == 'depression':\n",
        "      label_seq[i] = 0\n",
        "    else:\n",
        "      label_seq[i] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuZL50zpXeeE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b2666be-1fd9-4901-ce2c-729958daee88"
      },
      "source": [
        "bert_labels = []\n",
        "bert_posts = []\n",
        "for post,label in zip(posts,lables):\n",
        "  bert_labels.extend(label)\n",
        "  bert_posts.extend(post)\n",
        "print(len(bert_posts),len(bert_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14259 14259\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8ArXEzqZE_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94457c93-6c26-4662-f438-46b2953f0b0e"
      },
      "source": [
        "x = 8\n",
        "print(bert_posts[x],bert_labels[x])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It took me a few months to realize this but my pharmacist has been treating me very nice every time I visit to pickup my anti-depressant medication. She would always ask me how I’m doing and always smile at me. It’s not much but you don’t notice that with any other pharmacist. At first I found it odd that she was specifically being really nice to me but now I realize she was probably doing it because she knows I have depression and she wanted to cheer me up. So, to that pharmacist, I wanna thank you for that. You were the only person that interacts with me and it slightly lifts my mood when you treat me like a person. :) 0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}